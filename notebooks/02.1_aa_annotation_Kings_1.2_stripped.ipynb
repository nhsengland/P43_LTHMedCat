{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating documents using King's 1.2 finetuned model on LTH data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     \n",
    "    author: naa\n",
    "    created: 2023-05-11\n",
    "    version: 0.1.0\n",
    "\n",
    "    \n",
    "The steps in this notebook is similar to 01_aa_annotation.ipynb, with some changes due to differences in format (type-ids), additional information (concept filters applied, meta-annotation task) and missing information (semantic types) in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = \"/home/jovyan/nhsx_nlp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models and prepare MedCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = dir_root + \"models/kcl_private_modelpack/vocab.dat\"\n",
    "cdb_path = dir_root + \"models/kcl_private_modelpack/cdb.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model (King's 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDCAT_MODEL_PATH = Path(\n",
    "    dir_root + \"/models/kcl_private_modelpack\"\n",
    ")\n",
    "logging.debug(f\"Loading MedCAT models from {MEDCAT_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise cdb\n",
    "cdb = CDB()\n",
    "cdb = CDB.load(cdb_path)\n",
    "\n",
    "# Create and load the Vocabulary\n",
    "vocab = Vocab()\n",
    "vocab = Vocab.load(vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdb, len_cdb = common_setupConfig(cdb,MEDCAT_MODEL_PATH)\n",
    "len_cdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise meta annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_paths = [i for i in MEDCAT_MODEL_PATH.glob(\"meta_*\") if i.is_dir()]\n",
    "meta_cats = [MetaCAT.load(save_dir_path=meta_path) for meta_path in meta_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check meta-annotation tasks\n",
    "meta_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise CAT (main class from medcat used for concept annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat,ts = common_initialiseCAT(cdb, vocab, meta_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create typeid2name dictionary with key:value of typeid:semantic type name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add SNOMED-CT semantic tag to understand type_ids\n",
    "#this is a dictionary that contains the type ids and their names\n",
    "typeid2name = cat.cdb.addl_info['type_id2name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeid2name.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dir_root + \"/data/raw/neurology_letters_2023_03_18.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text  = \"He was diagnosed with a neurology issue\"\n",
    "doc = cat(text)\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.get_entities(\"He was diagnosed with a neurology issue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define subset of data to be analysed if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= df[0:] # take all of data to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test multiprocessing annotator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data =[(1,\"He was a neurology patient\")]\n",
    "results = cat.multiprocessing(in_data,nproc=2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "displacy.render(cat(in_data[0]), style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create data iterator for multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  data has to be in the form of a list of tuples containing two elements each: docid and doctext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out data according to sentence length if required\n",
    "#data = data[data.doctext.apply(lambda x: len(str(x))>10)] #select data with length of >10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data=[]\n",
    "for docid, row in data[['doctext']].iterrows():\n",
    "    #print(docid)\n",
    "    text=row['doctext']\n",
    "    in_data.append((docid,text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiprocess based on number of characters in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_chars = 500000 # define batch size before multiprocessing\n",
    "\n",
    "results = cat.multiprocessing(in_data, batch_size_chars=batch_size_chars, nproc=8) # try with small document number first if required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip this if above alternative step done - multiprocess based on number of documents (alternative way to multiprocessing based on number of characters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size to the number of documents\n",
    "batch_size = 100 # Batch size (BS) in number of documents\n",
    "\n",
    "# Run model\n",
    "if __name__ == '__main__':\n",
    "    import torch\n",
    "    torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "    results_pipe = cat.multiprocessing_pipe(in_data[:1000], # Formatted data\n",
    "                                       batch_size = batch_size,\n",
    "                                       nproc=2) # Increase it when having more cores available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check result of multiprocessing with King's model on one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of input data corresponds to the key of the annotated results, allowing for inspection of input text and extracted entitites\n",
    "data.iloc[3]['doctext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[3].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results[3].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check entities extracted for one document in structured annotation corpus\n",
    "for annotation in list(results[3]['entities'].values()): # change .values object is a list as dict values are not subscriptable\n",
    "    #print(annotation)\n",
    "    print((annotation['types'][0])) #to access type as just a string, access the first item in it if CUIs have one-one mapping to semantic type\n",
    "    print()\n",
    "    \n",
    "    #print(list(results[3]['entities'].values()))\n",
    "    #print(annotation['cui'],annotation['pretty_name'])\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save structured corpus of data annotated from  model: nhsx_nlp/models/kcl_private_modelpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = dir_root + '/data/interim/Annotated_Finetuned_Kings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results, open(DATA_DIR+'2023_05_12_Kings_untrained_annotated_results.dat','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatten nested dictionary of annotated results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load untrained structured annotation corpus if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open(DATA_DIR + '2023_05_12_Kings_untrained_annotated_results.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = {\n",
    "    'docid': [], # most superficial key in annotated results\n",
    "    'cui': [],\n",
    "    'pretty_name': [],\n",
    "    'source_value': [],\n",
    "    'detected_name': [],\n",
    "    'type': [],\n",
    "    'context_similarity': [],\n",
    "    'text': [], #pulled from doctext in raw data    \n",
    "    'Presence' : [], # Meta annotation of presence\n",
    "    'Subject': [], # Meta annotation of subject experiencing\n",
    "    'Time' : [] # Meta annotation of temporality of entity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create flattened dictionary of results for each entity and their locations, CUI, pretty_name, source value, semantic types, context similarity and meta-annotation tasks\n",
    "for doc in list(results.keys()): # the most superficial key in the nested dictionary of results is the document id\n",
    "\n",
    "    for entity in list(results[doc]['entities'].values()): # the key in the lower layer of the nested dictionary of the document is the entity, which has its own unique id \n",
    "        \n",
    "        flat_data['docid'].append(doc)\n",
    "        flat_data['cui'].append(entity['cui'])\n",
    "        flat_data['pretty_name'].append(entity['pretty_name'])\n",
    "        flat_data['source_value'].append(entity['source_value'])\n",
    "        flat_data['detected_name'].append(entity['detected_name'])\n",
    "        \n",
    "        flat_data['context_similarity'].append(entity['context_similarity'])\n",
    "        flat_data['Presence'].append(entity['meta_anns']['Presence']['value'])\n",
    "        flat_data['Subject'].append(entity['meta_anns']['Subject']['value'])\n",
    "        flat_data['Time'].append(entity['meta_anns']['Time']['value'])\n",
    "        flat_data['text'].append(data.iloc[doc]['doctext'])\n",
    "\n",
    "        if entity['types']: \n",
    "            # if list is not empty (a list with an element is True)\n",
    "            # for some reason, semantic type of an entity is in a list, but if we do (entity['types'][0]), we get an index out of range message, because there are CUIs that do not have semantic types\n",
    "            flat_data['type'].append(entity['types']) #need to access first element in the list entity['types'] if every CUI correspond to just one semantic type\n",
    "             \n",
    "        else:\n",
    "            flat_data['type'].append('Nil') # for CUIs with no semantic type, change to 'Nil'. This is a quirk of this model, whereby 7 CUIs in 76 documents do not have semantic types or type ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value length of each key and confirm they are the same before converting them to dataframe\n",
    "for i in flat_data.keys():\n",
    "    print(len(flat_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = pd.DataFrame.from_dict(flat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if CUIs map to more than 1 semantic type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listtype = []\n",
    "for typelist in df_flat.type:\n",
    "    if not typelist == 'Nil':\n",
    "        listtype.append(len(typelist))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(listtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_type = set(listtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_type # --> CUIs have a one-one relationship with semantic types. Therefore we can re-use the below code, and access the 0'th element in entity['types'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = {\n",
    "    'docid': [], # most superficial key in annotated results\n",
    "    'cui': [],\n",
    "    'pretty_name': [],\n",
    "    'source_value': [],\n",
    "    'detected_name': [],\n",
    "    'type': [],\n",
    "    'context_similarity': [],\n",
    "    'text': [], #pulled from doctext in raw data    \n",
    "    'Presence' : [], # Meta annotation of presence\n",
    "    'Subject': [], # Meta annotation of subject experiencing\n",
    "    'Time' : [] # Meta annotation of temporality of entity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create flattened dictionary of results for each entity and their locations, CUI, pretty_name, source value, semantic types,  context similarity and meta annotation\n",
    "for doc in list(results.keys()): # the most superficial key in the nested dictionary of results is the document id\n",
    "\n",
    "    for entity in list(results[doc]['entities'].values()): # the key in the lower layer of the nested dictionary of the document is the entity, which has its own unique id \n",
    "        \n",
    "        flat_data['docid'].append(doc)\n",
    "        flat_data['cui'].append(entity['cui'])\n",
    "        flat_data['pretty_name'].append(entity['pretty_name'])\n",
    "        flat_data['source_value'].append(entity['source_value'])\n",
    "        flat_data['detected_name'].append(entity['detected_name'])\n",
    "        \n",
    "        flat_data['context_similarity'].append(entity['context_similarity'])\n",
    "        flat_data['Presence'].append(entity['meta_anns']['Presence']['value'])\n",
    "        flat_data['Subject'].append(entity['meta_anns']['Subject']['value'])\n",
    "        flat_data['Time'].append(entity['meta_anns']['Time']['value'])\n",
    "        flat_data['text'].append(data.iloc[doc]['doctext'])\n",
    "\n",
    "        if entity['types']: \n",
    "            # if list is not empty (a list with an element is True)\n",
    "            # for some reason, semantic type of an entity is in a list, but if we do (entity['types'][0]), we get an index out of range message, because there are CUIs that do not have semantic types\n",
    "            flat_data['type'].append(entity['types'][0]) #need to access first element in the list entity['types']\n",
    "             \n",
    "        else:\n",
    "            flat_data['type'].append('Nil') # for CUIs with no semantic type, change to 'Nil'. This is a quirk of this model, whereby 7 CUIs in 76 documents do not have semantic types or type ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check value length of each key and confirm they are the same before converting them to dataframe\n",
    "for i in flat_data.keys():\n",
    "    print(len(flat_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = pd.DataFrame.from_dict(flat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save flattened annotated results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(dir_root + '/data/interim/Annotated_Finetuned_Kings/2023_05_12_Kings_untrained_flattened_annotated_results.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df_flat.to_csv(filepath, index=False) #put index = False so we don't get an unnamed column when we pd.read_csv this csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload data saved above for inspection if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data_path = dir_root + '/data/interim/Annotated_Finetuned_Kings/2023_05_12_Kings_untrained_flattened_annotated_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = pd.read_csv(flat_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat['type'].unique() # so this confirms that there are CUIs that do not have semantic types, and we have coded that as 'Nil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which CUIs don't have type ids?\n",
    "no_type = df_flat.query(\"type == 'Nil'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_type.nunique() # there are 7 CUIS that do not have semantic types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_type.cui.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_type.pretty_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dict of CUIs and their location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be a map from CUI to a list of documents where it appears: {\"cui\": [<doc_id>, <doc_id>, ...], ..}\n",
    "cui_location = {}\n",
    "for doc in list(results.keys()):\n",
    "    for annotation in list(results[doc]['entities'].values()):\n",
    "        if annotation['cui'] in cui_location:\n",
    "            cui_location[annotation['cui']].append(doc)\n",
    "        else:\n",
    "            cui_location[annotation['cui']] = [doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that number length of dictionary corresponds to total number of concepts extracted\n",
    "len(cui_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dict of type ids and their location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test CUI conversion to type ID\n",
    "cat.cdb.cui2type_ids['3457005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the type_ids and their corresponding documents\n",
    "# Let's also save the type_ids location\n",
    "type_ids_location = {}\n",
    "\n",
    "for cui in cui_location.keys():\n",
    "\n",
    "      if list(cat.cdb.cui2type_ids[cui]): # If CUI has type id, then...\n",
    "\n",
    "         type_ids_location[list(cat.cdb.cui2type_ids[cui])[0]] = cui_location[cui] # assign the location of that type id as its CUI\n",
    "\n",
    "      else:\n",
    "         type_ids_location['No type'] = cui_location[cui] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check length of this type id location dictionary is the same as unique values of types\n",
    "len(type_ids_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dict of CUIS and their context similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be a map from CUI its context similarities : {\"cui\": [<context similarity>, <context similarity>, ...], ..}\n",
    "cui_similarity = {}\n",
    "\n",
    "for doc in list(results.keys()):\n",
    "    for annotation in list(results[doc]['entities'].values()):\n",
    "        if annotation['cui'] in cui_similarity:\n",
    "         cui_similarity[annotation['cui']].append(annotation['context_similarity'])\n",
    "        else:\n",
    "         cui_similarity[annotation['cui']] = [annotation['context_similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check length of this context similarity dictionary is the same as unique numbers of context similarity in flat dataframe\n",
    "len(cui_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save untrained CUI location, type ID location and CUI_context similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data so that we don't have to do the annotation again\n",
    "pickle.dump(cui_location, open(DATA_DIR + \"cui_location.dat\", 'wb'))\n",
    "pickle.dump(type_ids_location, open(DATA_DIR + \"type_ids_location.dat\", 'wb'))\n",
    "pickle.dump(cui_similarity, open(DATA_DIR + \"cui_similarity.dat\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load untrained annotated results from  King's finetuned SNOMED model on Data, CUI location, type id location and context similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = dir_root + '/data/interim/Annotated_Finetuned_Kings/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load structured annotation dataset\n",
    "untrained_results = pickle.load(open(DATA_DIR + '2023_05_12_Kings_untrained_annotated_results.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check loaded data\n",
    "untrained_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_location = pickle.load(open(DATA_DIR + 'cui_location.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ids_location = pickle.load(open(DATA_DIR + 'type_ids_location.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_similarity = pickle.load(open(DATA_DIR + 'cui_similarity.dat','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise annotation frequency for finetuned KCH model we have not trained for LTH's data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create df of CUI, number of documents containing it, type ids, cui similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_ndocs = [('cui', 'ndocs')]\n",
    "\n",
    "for cui in cui_location.keys():\n",
    "    cui_ndocs.append((cui, len(cui_location[cui])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cui_ndocs = pd.DataFrame(cui_ndocs[1:], columns=cui_ndocs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cui_ndocs.head() # ndocs == number of documents containing that CUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cat.cdb.cui2type_ids.get with CUIs that don't have type ids\n",
    "cat.cdb.cui2type_ids['34281000175105'] # --> the output is 'set()'??? Buggy. empty set basically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.cdb.cui2type_ids.get('34281000175105', 'unk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cat.cdb.cui2type_ids.get with CUIs that do have type ids\n",
    "cat.cdb.cui2type_ids['3457005'] # --> output is a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add type ids for each CUI\n",
    "\n",
    "df_cui_ndocs['type_ids'] = ['unk'] * len(df_cui_ndocs) # unknown token times the length of dataframe\n",
    "cols = list(df_cui_ndocs.columns)\n",
    "\n",
    "for i in range(len(df_cui_ndocs)):\n",
    "    cui = df_cui_ndocs.iat[i, cols.index('cui')] # access the cui in the 'i'th row and index of column 'cui'\n",
    "\n",
    "    if  cat.cdb.cui2type_ids.get(cui, 'unk'): # if this set has elements, then...\n",
    "        type_ids = tuple(cat.cdb.cui2type_ids.get(cui, 'unk'))[0] #Turn the 'set' type of type ids into a tuple then access the string of type id in it for us to easily filter the type \n",
    "    else:\n",
    "        type_ids = '00000' # CUIs with no type IDs, get given a value of '00000'\n",
    "\n",
    "    df_cui_ndocs.iat[i, cols.index('type_ids')] = type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add type id semantic tag for each CUI\n",
    "semantic_tag =[]\n",
    "for i, row in df_cui_ndocs.iterrows():\n",
    "     \n",
    "    if row['type_ids'] == '00000':\n",
    "        semantic_tag.append('Nil')\n",
    "    else:\n",
    "        type = row['type_ids'] # change set to row and access type id in it\n",
    "        semantic_tag.append(typeid2name[type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cui_ndocs['Semantic_tags'] = semantic_tag #make the semantic tag list into a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Semantic tags as column in new dataframe\n",
    "df_cui_ndocs.loc[df_cui_ndocs['Semantic_tags'] == 'disorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add name for each CUI\n",
    "df_cui_ndocs['name'] = ['unk'] * len(df_cui_ndocs)\n",
    "cols = list(df_cui_ndocs.columns)\n",
    "for i in range(len(df_cui_ndocs)):\n",
    "    cui = df_cui_ndocs.iat[i, cols.index('cui')]\n",
    "    name = cat.cdb.cui2preferred_name.get(cui, 'unk')\n",
    "    df_cui_ndocs.iat[i, cols.index('name')] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the percentage column\n",
    "total_docs = len(data)\n",
    "df_cui_ndocs['perc_docs'] = (df_cui_ndocs['ndocs'] / total_docs) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mean of context similarity per cui \n",
    "cons_similarity = []\n",
    "for _, row in df_cui_ndocs.iterrows():\n",
    "    cui = row['cui']\n",
    "    #print(cui)\n",
    "    mean_similarity = statistics.mean(cui_similarity[cui])\n",
    "    cons_similarity.append(mean_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cui_ndocs['mean_similarity'] = cons_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cui_ndocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cui_ndocs = df_cui_ndocs.sort_values('ndocs', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cui_ndocs.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe of CUIs, number of docs mentioning them, related semantic tags, concept name, and mean context similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(dir_root + '/data/interim/Annotated_Finetuned_Kings/2023_05_15_cui_docs.csv')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df_cui_ndocs.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot unfiltered concepts extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file again if required as per filepath above\n",
    "df_cui_ndocs = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cui_ndocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 30 concepts\n",
    "sns.reset_defaults()\n",
    "sns.set(\n",
    "    rc={'figure.figsize':(5,15)}, \n",
    "    style=\"whitegrid\",\n",
    "    palette='pastel'\n",
    ")\n",
    "f, ax = plt.subplots()\n",
    "_data = df_cui_ndocs.iloc[0:30]\n",
    "sns.barplot(x=\"ndocs\", y=\"name\", data=_data, label=\"Concept\", color=\"b\")\n",
    "_ = ax.set(xlim=(0, 6000), ylabel=\"SNOMED-CT concept\", xlabel=\"Count of documents with mention of concept\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plot top 30 SNOMED-CT concepts for type T-11: Disorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 30 concepts\n",
    "sns.reset_defaults()\n",
    "sns.set(\n",
    "    rc={'figure.figsize':(5,15)}, \n",
    "    style=\"whitegrid\",\n",
    "    palette='pastel'\n",
    ")\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "# Subset the data and chose only T-11(Disorder), top 30\n",
    "_data = df_cui_ndocs[df_cui_ndocs['type_ids'].apply(lambda x: 'T-11' in x)].iloc[:31]\n",
    "\n",
    "sns.barplot(x=\"ndocs\", y=\"name\", data=_data, label=\"Concept\", color=\"b\")\n",
    "_ = ax.set(xlim=(0, 1200), ylabel=\"SNOMED-CT concept\", xlabel=\"Count of documents with mention of concept\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot for percentage of documents with mention of concept\n",
    "sns.reset_defaults()\n",
    "sns.set(\n",
    "    rc={'figure.figsize':(6,15)}, \n",
    "    style=\"whitegrid\",\n",
    "    palette='pastel',\n",
    "    )\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "# Subset the data and chose only 9090192, top 30\n",
    "_data = df_cui_ndocs[df_cui_ndocs['type_ids'].apply(lambda x: 'T-11' in x)].iloc[:31]\n",
    "\n",
    "sns.barplot(x=\"perc_docs\", y=\"name\", data=_data, label=\"Concept Name\", color=\"b\")\n",
    "_ = ax.set(xlim=(0, 1.5), ylabel=\"Concept Name\", xlabel=\"Percentage of documents with mention of concept \\n for King's 1.2 model annotation \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) \n[GCC 10.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
